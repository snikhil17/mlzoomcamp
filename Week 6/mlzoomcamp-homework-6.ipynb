{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>This Week Questions</u>\n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 1:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nLet's train a decision tree regressor to predict the price variable.\n\nTrain a model with max_depth=1\n</ul>   \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 2:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nTrain a random forest model with these parameters:n_estimators=10 random_state=1 n_jobs=-1 (optional - to make training faster)\n    \nWhat's the RMSE of this model on validation?\n</ul>     \n    \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 3:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's experiment with the n_estimators parameter\n    \nTry different values of this parameter from 10 to 200 with step 10\n\nSet random_state to 1\n    \nEvaluate the model on the validation dataset\n    \nAfter which value of n_estimators does RMSE stop improving?\n</ul> \n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 4:</u></p>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nLet's select the best max_depth:\n\nTry different values of max_depth: [10, 15, 20, 25]\n    \nFor each of these values, try different values of n_estimators from 10 till 200 (with step 10)\n    \nFix the random seed: random_state=1\n    \n\nAfter which value of n_estimators does RMSE stop improving?\n</ul>\n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Bonus Question: Not Graded</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nWill the answer be different if we change the seed for the model?\n    \n</ul>   \n    \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 5:</u></p>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nWe can extract feature importance information from tree-based models.\n\nAt each step of the decision tree learning algorith, it finds the best split. When doint it, we can calculate \"gain\" - the reduction in impurity before and after the split. This gain is quite useful in understanding what are the imporatant features for tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the feature_importances_ field.\n\nFor this homework question, we'll find the most important feature:\n\nTrain the model with these parametes:\nn_estimators=10,\nmax_depth=20,\nrandom_state=1,\nn_jobs=-1 (optional)\n    \nGet the feature importance information from this model\n    \n\nWhat's the most important feature?\n</ul>\n      \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 6:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's train an XGBoost model! For this question, we'll tune the eta parameter\n\nInstall XGBoost\n    \nCreate DMatrix for train and validation\n    \nCreate a watchlist\n    \nTrain a model with these parameters for 100 rounds:\n\nxgb_params = {'eta': 0.3, \n'max_depth': 6,\n'min_child_weight': 1,\n\n'objective': 'reg:squarederror',\n'nthread': 8,\n\n'seed': 1,\n'verbosity': 1,\n}\n    \nNow change eta first to 0.1 and then to 0.01\n    \n\nWhich eta leads to the best RMSE score on the validation dataset?    \n</ul>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.tree import DecisionTreeRegressor, export_text, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn import metrics\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:36:39.452107Z","iopub.execute_input":"2021-10-18T05:36:39.452604Z","iopub.status.idle":"2021-10-18T05:36:39.462537Z","shell.execute_reply.started":"2021-10-18T05:36:39.452569Z","shell.execute_reply":"2021-10-18T05:36:39.461465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [\n    'neighbourhood_group', 'room_type', 'latitude', 'longitude',\n    'minimum_nights', 'number_of_reviews','reviews_per_month',\n    'calculated_host_listings_count', 'availability_365',\n    'price'\n]\n\ndf = pd.read_csv('../input/new-york-city-airbnb-open-data/AB_NYC_2019.csv', usecols=columns)\ndf.reviews_per_month = df.reviews_per_month.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:36:39.878001Z","iopub.execute_input":"2021-10-18T05:36:39.878287Z","iopub.status.idle":"2021-10-18T05:36:40.014688Z","shell.execute_reply.started":"2021-10-18T05:36:39.878254Z","shell.execute_reply":"2021-10-18T05:36:40.01367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nLook at the price variable. Does it have a long tail?\n\"\"\"\nfig, axes = plt.subplots(1, 2)\n      \naxes[0].hist(df['price'], density= True, bins = 50)\naxes[0].set_title(\"Normal scale Price\")\n\naxes[1].hist(np.log1p(df['price']), density= True, bins = 50)\naxes[1].set_title(\"Logarithmic scale Price\");","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:37:05.0018Z","iopub.execute_input":"2021-10-18T05:37:05.002076Z","iopub.status.idle":"2021-10-18T05:37:05.631072Z","shell.execute_reply.started":"2021-10-18T05:37:05.002047Z","shell.execute_reply":"2021-10-18T05:37:05.629963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:37:05.633002Z","iopub.execute_input":"2021-10-18T05:37:05.633338Z","iopub.status.idle":"2021-10-18T05:37:05.658818Z","shell.execute_reply.started":"2021-10-18T05:37:05.633293Z","shell.execute_reply":"2021-10-18T05:37:05.657953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Apply the log transformation to the price variable using the np.log1p() function.\"\"\"\ny_train = np.log1p(df_train['price'].values)\ny_val = np.log1p(df_val['price'].values)\ny_test = np.log1p(df_test['price'].values)\n\n\"\"\"Make sure that the target value ('price') is not in your dataframe.\"\"\"\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:45:26.064523Z","iopub.execute_input":"2021-10-18T04:45:26.064884Z","iopub.status.idle":"2021-10-18T04:45:26.072599Z","shell.execute_reply.started":"2021-10-18T04:45:26.064855Z","shell.execute_reply":"2021-10-18T04:45:26.071815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Now, use DictVectorizer to turn train and validation into matrices:\"\"\"\ndv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train.to_dict(orient='records')\nval_dict = df_val.to_dict(orient='records')\n\nX_train = dv.fit_transform(train_dict)\nX_val = dv.transform(val_dict)\n\ndv_features = dv.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:45:26.073875Z","iopub.execute_input":"2021-10-18T04:45:26.07421Z","iopub.status.idle":"2021-10-18T04:45:26.94301Z","shell.execute_reply.started":"2021-10-18T04:45:26.074181Z","shell.execute_reply":"2021-10-18T04:45:26.942114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 1:</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nLet's train a decision tree regressor to predict the price variable.\n\nTrain a model with max_depth=1\n\n\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"\ndt_r = DecisionTreeRegressor(max_depth=1)\ndt_r.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(export_text(dt_r, feature_names=dv_features))\nprint('\\n')\nplot_tree(dt_r , filled = True , feature_names = dv_features);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-1: <b>room_type</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 2:</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nTrain a random forest model with these parameters:n_estimators=10 random_state=1 n_jobs=-1 (optional - to make training faster)\n    \nWhat's the RMSE of this model on validation?\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"rf_r = RandomForestRegressor(n_estimators=10, random_state=1, n_jobs=-1)\nrf_r.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rf_r.predict(X_val)\nrmse_val = metrics.mean_squared_error(y_val, y_pred, squared = False)\nrmse_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-2: <b>0.459</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 3:</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's experiment with the n_estimators parameter\n    \nTry different values of this parameter from 10 to 200 with step 10\n\nSet random_state to 1\n    \nEvaluate the model on the validation dataset\n    \nAfter which value of n_estimators does RMSE stop improving?\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"preds = []\nrmse = {}\nfor est in range(10,201,10):\n    rf = RandomForestRegressor(n_estimators=est, random_state=1)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_val)\n    preds.append(y_pred)\n    rmse_val = metrics.mean_squared_error(y_val, y_pred, squared = False)\n    rmse[est] = rmse_val\n    print(f\"n_estimators = {est} RMSE: {rmse[est]}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:43:22.979049Z","iopub.execute_input":"2021-10-18T05:43:22.979331Z","iopub.status.idle":"2021-10-18T05:43:27.31255Z","shell.execute_reply.started":"2021-10-18T05:43:22.979302Z","shell.execute_reply":"2021-10-18T05:43:27.311216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.plot(rmse.keys(), rmse.values())\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE')\nplt.xticks(list(rmse.keys()))\nplt.title('RandomForest RMSE over various n_estimators');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-3: <b>120</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 4:</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nLet's select the best max_depth:\n\nTry different values of max_depth: [10, 15, 20, 25]\n    \nFor each of these values, try different values of n_estimators from 10 till 200 (with step 10)\n    \nFix the random seed: random_state=1\n    \n    \nWhat's the best max_depth?\n</ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"scores = []\n\nfor d in [10, 15, 20, 25]:\n    for n in range(10,201,10):\n        rf = RandomForestRegressor(n_estimators=n,\n                                    max_depth=d,\n                                    random_state=1)\n        rf.fit(X_train, y_train)\n\n        y_pred = rf.predict(X_val)\n        rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n        print(f\"max_depth = {d}, n_estimators = {n}, RMSE: {rmse}\")\n        scores.append((d, n, rmse))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['max_depth', 'n_estimators', 'rmse']\ndf_scores = pd.DataFrame(scores, columns=columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for d in [10, 15, 20, 25]:\n    df_subset = df_scores[df_scores.max_depth == d]\n    \n    plt.plot(df_subset.n_estimators, df_subset.rmse,\n             label='max_depth=%d' % d)\n\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best value for max_depth: {int(df_scores.max_depth[df_scores.rmse == min(df_scores.rmse)].values)}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-4: <b>15</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Bonus question (not graded):</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nWill the answer be different if we change the seed for the model?\n    \n</ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"scores = []\n\nfor d in [10, 15, 20, 25]:\n    for n in range(10,201,10):\n        rf = RandomForestRegressor(n_estimators=n,\n                                    max_depth=d,\n                                    random_state=7)\n        rf.fit(X_train, y_train)\n\n        y_pred = rf.predict(X_val)\n        rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n        print(f\"max_depth = {d}, n_estimators = {n}, RMSE: {rmse}\")\n        scores.append((d, n, rmse))\n        \ncolumns = ['max_depth', 'n_estimators', 'rmse']\ndf_scores = pd.DataFrame(scores, columns=columns) \n\n\nfor d in [10, 15, 20, 25]:\n    df_subset = df_scores[df_scores.max_depth == d]\n    \n    plt.plot(df_subset.n_estimators, df_subset.rmse,\n             label='max_depth=%d' % d)\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:08:48.505226Z","iopub.execute_input":"2021-10-18T05:08:48.505495Z","iopub.status.idle":"2021-10-18T05:28:17.500718Z","shell.execute_reply.started":"2021-10-18T05:08:48.505466Z","shell.execute_reply":"2021-10-18T05:28:17.499895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-Bonus: <b>No change still 15</b></div>","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 5:</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nWe can extract feature importance information from tree-based models.\n\nAt each step of the decision tree learning algorith, it finds the best split. When doint it, we can calculate \"gain\" - the reduction in impurity before and after the split. This gain is quite useful in understanding what are the imporatant features for tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the feature_importances_ field.\n\nFor this homework question, we'll find the most important feature:\n\nTrain the model with these parametes:\nn_estimators=10,\nmax_depth=20,\nrandom_state=1,\nn_jobs=-1 (optional)\nGet the feature importance information from this model\n    \n    \nWhat's the most important feature?\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=10,\n    max_depth=20,\n    random_state=1,\n    n_jobs=-1)\nrf.fit(X_train, y_train)\nfeature_importance = rf.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_feat_imp = [(feat,imp) for feat, imp in zip(dv_features, feature_importance)]\ncolumns = ['features', 'feature_importance']\ndf_feat_importance = pd.DataFrame(list_feat_imp, columns=columns) \ndf_feat_importance.sort_values(by='feature_importance', ascending=False, inplace=True)\n\n\nplt.figure(figsize= (16,10))\nax= sns.barplot( df_feat_importance['feature_importance'], df_feat_importance['features'],palette='copper' )\nplt.title(\"Feature Importance\",fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-5: <b>room_type=Entire home/apt</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 6:</b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's train an XGBoost model! For this question, we'll tune the eta parameter\n\nInstall XGBoost\n    \nCreate DMatrix for train and validation\n    \nCreate a watchlist\n    \nTrain a model with these parameters for 100 rounds:\n\nxgb_params = {'eta': 0.3, \n'max_depth': 6,\n'min_child_weight': 1,\n\n'objective': 'reg:squarederror',\n'nthread': 8,\n\n'seed': 1,\n'verbosity': 1,\n}\n    \nNow change eta first to 0.1 and then to 0.01\n\nWhich eta leads to the best RMSE score on the validation dataset?\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"def parse_xgb_output(output):\n    results = []\n\n    for line in output.stdout.strip().split('\\n'):\n        it_line, train_line, val_line = line.split('\\t')\n\n        it = int(it_line.strip('[]'))\n        train = float(train_line.split(':')[1])\n        val = float(val_line.split(':')[1])\n\n        results.append((it, train, val))\n    \n    columns = ['num_iter', 'train_rmse', 'val_rmse']\n    df_results = pd.DataFrame(results, columns=columns)\n    return df_results","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:45:26.94445Z","iopub.execute_input":"2021-10-18T04:45:26.944699Z","iopub.status.idle":"2021-10-18T04:45:26.952538Z","shell.execute_reply.started":"2021-10-18T04:45:26.944671Z","shell.execute_reply":"2021-10-18T04:45:26.951662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=dv_features)\ndval = xgb.DMatrix(X_val, label=y_val, feature_names=dv_features)\nwatchlist = [(dtrain, 'train'), (dval, 'val')]\n\nxgb_params = {\n    'eta':0.3,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n    'seed': 1,\n    'verbosity': 1,\n}\n\nscores = dict()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:43.914386Z","iopub.execute_input":"2021-10-18T04:46:43.914717Z","iopub.status.idle":"2021-10-18T04:46:43.932537Z","shell.execute_reply.started":"2021-10-18T04:46:43.914674Z","shell.execute_reply":"2021-10-18T04:46:43.931681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture output\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=5,\n                  evals=watchlist)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:44.904212Z","iopub.execute_input":"2021-10-18T04:46:44.905297Z","iopub.status.idle":"2021-10-18T04:46:47.263423Z","shell.execute_reply.started":"2021-10-18T04:46:44.905254Z","shell.execute_reply":"2021-10-18T04:46:47.262693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = f'eta={xgb_params[\"eta\"]}'\nscores[key] = parse_xgb_output(output)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:47.265204Z","iopub.execute_input":"2021-10-18T04:46:47.265735Z","iopub.status.idle":"2021-10-18T04:46:47.271319Z","shell.execute_reply.started":"2021-10-18T04:46:47.265688Z","shell.execute_reply":"2021-10-18T04:46:47.270482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params[\"eta\"] = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:47.272685Z","iopub.execute_input":"2021-10-18T04:46:47.272929Z","iopub.status.idle":"2021-10-18T04:46:47.285555Z","shell.execute_reply.started":"2021-10-18T04:46:47.2729Z","shell.execute_reply":"2021-10-18T04:46:47.284675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture output\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=5,\n                  evals=watchlist)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:47.582676Z","iopub.execute_input":"2021-10-18T04:46:47.583348Z","iopub.status.idle":"2021-10-18T04:46:50.143139Z","shell.execute_reply.started":"2021-10-18T04:46:47.583298Z","shell.execute_reply":"2021-10-18T04:46:50.142182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = f'eta={xgb_params[\"eta\"]}'\nscores[key] = parse_xgb_output(output)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:50.14514Z","iopub.execute_input":"2021-10-18T04:46:50.145477Z","iopub.status.idle":"2021-10-18T04:46:50.151996Z","shell.execute_reply.started":"2021-10-18T04:46:50.145434Z","shell.execute_reply":"2021-10-18T04:46:50.15098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params[\"eta\"] = 0.01","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:50.153604Z","iopub.execute_input":"2021-10-18T04:46:50.153938Z","iopub.status.idle":"2021-10-18T04:46:50.167008Z","shell.execute_reply.started":"2021-10-18T04:46:50.153895Z","shell.execute_reply":"2021-10-18T04:46:50.166034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture output\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=5,\n                  evals=watchlist)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:50.16907Z","iopub.execute_input":"2021-10-18T04:46:50.169636Z","iopub.status.idle":"2021-10-18T04:46:52.722325Z","shell.execute_reply.started":"2021-10-18T04:46:50.169585Z","shell.execute_reply":"2021-10-18T04:46:52.721456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = f'eta={xgb_params[\"eta\"]}'\nscores[key] = parse_xgb_output(output)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:46:52.723529Z","iopub.execute_input":"2021-10-18T04:46:52.724174Z","iopub.status.idle":"2021-10-18T04:46:52.72857Z","shell.execute_reply.started":"2021-10-18T04:46:52.72414Z","shell.execute_reply":"2021-10-18T04:46:52.728037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for eta, df_score in scores.items():\n    plt.plot(df_score.num_iter, df_score.val_rmse, label=eta)\n\n    plt.xlabel('n_rounds')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.title('Full graph')\n\nplt.suptitle(f'XGBoost [eta] hypertuning');","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:49:02.95468Z","iopub.execute_input":"2021-10-18T04:49:02.954977Z","iopub.status.idle":"2021-10-18T04:49:03.449594Z","shell.execute_reply.started":"2021-10-18T04:49:02.954946Z","shell.execute_reply":"2021-10-18T04:49:03.448658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for eta, df_score in scores.items():\n    plt.plot(df_score.num_iter, df_score.val_rmse, label=eta)\n\n    plt.xlabel('n_rounds')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.title('Full graph')\n    plt.ylim(0.43,0.45)\n\nplt.suptitle(f'XGBoost [eta] hypertuning');","metadata":{"execution":{"iopub.status.busy":"2021-10-18T04:50:59.087818Z","iopub.execute_input":"2021-10-18T04:50:59.088131Z","iopub.status.idle":"2021-10-18T04:50:59.434355Z","shell.execute_reply.started":"2021-10-18T04:50:59.088084Z","shell.execute_reply":"2021-10-18T04:50:59.433375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-6: <b>0.1</b></div>","metadata":{}}]}