{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>This Week Questions</u>\n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 1:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>What is the most frequent observation (mode) for the column 'neighbourhood_group'?</li>\n</ul>   \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 2:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Create the correlation matrix for the numerical features of your train dataset. </li>\n<ul>\n<li>In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset. </li>    \n</ul> \n<li>What are the two features that have the biggest correlation in this dataset? </li>   \n    \n</ul>     \n    \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 3:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Calculate the mutual information score with the (binarized) price for the two categorical variables that we have. Use the training set only.</li>\n<li>Which of these two variables has bigger score?</li>\n<li>Round it to 2 decimal digits using round(score, 2)</li>\n</ul> \n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 4:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Now let's train a logistic regression</li>\n<li>Remember that we have two categorical variables in the data. Include them using one-hot encoding.</li>\n<li>Fit the model on the training dataset.</li>\n<ul>\n<li>To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:</li> \n<li>model = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)</li>\n</ul>     \n<li>Calculate the accuracy on the validation dataset and rount it to 2 decimal digits.</li>\n</ul>    \n    \n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 5:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>We have 9 features: 7 numerical features and 2 categorical.</li>\n<li>Let's find the least useful one using the feature elimination technique.</li>\n<li>Train a model with all these features (using the same parameters as in Q4).</li>\n<li>Now exclude each feature from this set and train a model without it. Record the accuracy for each model.</li>\n<li>For each feature, calculate the difference between the original accuracy and the accuracy without the feature.</li>\n<li>Which of following feature has the smallest difference? </li>\n<ul>\n<li>neighbourhood_group</li> \n<li>room_type</li>\n<li>number_of_reviews</li> \n<li>reviews_per_month</li>\n</ul> \n</ul>      \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 6:</u></p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>For this question, we'll see how to use a linear regression model from Scikit-Learn</li>\n<li>We'll need to use the original column 'price'. Apply the logarithmic transformation to this column.</li>\n<li>Fit the Ridge regression model on the training data.</li>\n<li>This model has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10]</li>\n<li>Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.</li>\n</ul>\n\n</div>\n","metadata":{"id":"pEYvPy4MR8XL"}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Importing Libraries</div>","metadata":{"id":"sFHMldcMR8XO"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mutual_info_score, mean_squared_error\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression, Ridge\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"bfrNH5ZtR8XP","execution":{"iopub.status.busy":"2021-09-29T12:14:23.583640Z","iopub.execute_input":"2021-09-29T12:14:23.584016Z","iopub.status.idle":"2021-09-29T12:14:24.924185Z","shell.execute_reply.started":"2021-09-29T12:14:23.583924Z","shell.execute_reply":"2021-09-29T12:14:24.923532Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Loading and Reading Data </div>","metadata":{"id":"6AcmLqEnR8XQ"}},{"cell_type":"code","source":"data = pd.read_csv('../input/new-york-city-airbnb-open-data/AB_NYC_2019.csv')\ndata.head()\n\n","metadata":{"id":"k7Kg5w00R8XR","outputId":"58e0f480-9bd1-4957-b21e-bc8691ccc6c3","execution":{"iopub.status.busy":"2021-09-29T12:14:24.925524Z","iopub.execute_input":"2021-09-29T12:14:24.926300Z","iopub.status.idle":"2021-09-29T12:14:25.367599Z","shell.execute_reply.started":"2021-09-29T12:14:24.926266Z","shell.execute_reply":"2021-09-29T12:14:25.366794Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Features used for this Project</div>","metadata":{"id":"LO5YvZUXR8XT"}},{"cell_type":"code","source":"features  = [\n    'neighbourhood_group',\n    'room_type',\n    'latitude',\n    'longitude',\n    'price',\n    'minimum_nights',\n    'number_of_reviews',\n    'reviews_per_month',\n    'calculated_host_listings_count',\n    'availability_365'\n]\ndf = data[features]\n\"\"\"Checking description of this project's data set\"\"\"\ndf.describe()","metadata":{"id":"2_tY-ufER8XU","outputId":"4540c40b-f00a-42c0-8bb2-3ceaabe23c6d","execution":{"iopub.status.busy":"2021-09-29T12:14:25.368664Z","iopub.execute_input":"2021-09-29T12:14:25.368899Z","iopub.status.idle":"2021-09-29T12:14:25.429234Z","shell.execute_reply.started":"2021-09-29T12:14:25.368873Z","shell.execute_reply":"2021-09-29T12:14:25.428329Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Missing Values? Impute them with 0</div>","metadata":{"id":"lRNTMkohR8XV"}},{"cell_type":"code","source":"from IPython.display import display\nmissing_vals = df.isnull().sum()\nprint(\"Before Imputing Missing Values\")\ndisplay(missing_vals.to_frame().reset_index().rename({'index': 'Variables', 0: 'Missing Values'}, axis = 1).sort_values(by = 'Missing Values', ascending = False).style.background_gradient('Reds'))\n\n\ndf.fillna(0, inplace = True)\nprint(\"After Imputing Missing Values\")\ndisplay(df.isnull().sum().to_frame().reset_index().rename({'index': 'Variables', 0: 'Missing Values'}, axis = 1).style.background_gradient('Reds'))\n","metadata":{"id":"H2NhQ18ZR8XW","outputId":"f13bfb83-010e-44af-9b6c-6ed74a0b98cc","execution":{"iopub.status.busy":"2021-09-29T12:14:25.431677Z","iopub.execute_input":"2021-09-29T12:14:25.433401Z","iopub.status.idle":"2021-09-29T12:14:25.563462Z","shell.execute_reply.started":"2021-09-29T12:14:25.433366Z","shell.execute_reply":"2021-09-29T12:14:25.562646Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:white\">Q1. What is the most frequent observation (mode) for the column 'neighbourhood_group'?</div>","metadata":{"id":"7S9_rm__R8XW"}},{"cell_type":"code","source":"print(\"Mode for variable 'neighbourhood_group': %s\" %(df['neighbourhood_group'].value_counts().head(1)))","metadata":{"id":"1MIBso-xR8XX","outputId":"bd4b3b7d-5c5f-4f45-d9d8-93ba95f08962","execution":{"iopub.status.busy":"2021-09-29T12:14:25.564571Z","iopub.execute_input":"2021-09-29T12:14:25.564896Z","iopub.status.idle":"2021-09-29T12:14:25.578656Z","shell.execute_reply.started":"2021-09-29T12:14:25.564867Z","shell.execute_reply":"2021-09-29T12:14:25.577840Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Split the data\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Split your data in train/val/test sets, with 60%/20%/20% distribution.</li>\n<li>Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.</li>\n<li>Make sure that the target value ('price') is not in your dataframe.</li>\n</ul>\n</div>\n","metadata":{"id":"d9zBkFZfR8XX"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n","metadata":{"id":"TDcd-ut6R8XY","execution":{"iopub.status.busy":"2021-09-29T12:14:25.579969Z","iopub.execute_input":"2021-09-29T12:14:25.580209Z","iopub.status.idle":"2021-09-29T12:14:25.609793Z","shell.execute_reply.started":"2021-09-29T12:14:25.580182Z","shell.execute_reply":"2021-09-29T12:14:25.609069Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 2:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Create the correlation matrix for the numerical features of your train dataset. </li>\n<ul>\n<li>In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset. </li>    \n</ul> \n<li>What are the two features that have the biggest correlation in this dataset? </li>   \n    \n</ul> \n</div>\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T09:24:56.466732Z","iopub.execute_input":"2021-09-17T09:24:56.467046Z","iopub.status.idle":"2021-09-17T09:24:56.527484Z","shell.execute_reply.started":"2021-09-17T09:24:56.467017Z","shell.execute_reply":"2021-09-17T09:24:56.526372Z"},"id":"uUu8RX24R8XY"}},{"cell_type":"code","source":"\"\"\"Creating List of Numerical and Categorical columns\"\"\"\ncategorical = [col for col in df.columns if df[col].dtype == 'object']\nnumerical = [col for col in df.columns if col not in categorical]\n\n\"\"\"Correlation of Numerical Columns\"\"\"\ndisplay(df[numerical].corr())\n\n\"\"\"Heatmap of Numerical Variables\"\"\"\ndisplay(sns.heatmap(df[numerical].corr(), annot = True, lw = 0.2, square = True, cmap = 'crest'))\n","metadata":{"id":"YqfYiZx7R8XZ","outputId":"b80cbf9f-1ec2-4dee-bffb-58536af01e56","execution":{"iopub.status.busy":"2021-09-29T12:14:25.611004Z","iopub.execute_input":"2021-09-29T12:14:25.611390Z","iopub.status.idle":"2021-09-29T12:14:26.465806Z","shell.execute_reply.started":"2021-09-29T12:14:25.611350Z","shell.execute_reply":"2021-09-29T12:14:26.464842Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Highest Correlation is between\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>reviews_per_month and number_of_reviews: 0.589407</li>\n</ul>\n</div>","metadata":{"id":"hPZj64Lucz7s"}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Make price binary\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>We need to turn the price variable from numeric into binary.</li>\n<li>Let's create a variable above_average which is 1 if the price is above (or equal to) 152.</li>\n</ul>\n</div>","metadata":{"id":"2Tg0JxEDZeWW"}},{"cell_type":"code","source":"\"\"\"price variable from numeric into binary.\"\"\"\ndf_full_train['above_average'] = (df_full_train['price'] >= 152).values.astype(int)\n","metadata":{"id":"1ya3UedmR8XZ","execution":{"iopub.status.busy":"2021-09-29T12:14:26.467593Z","iopub.execute_input":"2021-09-29T12:14:26.468086Z","iopub.status.idle":"2021-09-29T12:14:26.476156Z","shell.execute_reply.started":"2021-09-29T12:14:26.468040Z","shell.execute_reply":"2021-09-29T12:14:26.474963Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 3:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Calculate the mutual information score with the (binarized) price for the two categorical variables that we have. Use the training set only.</li>\n<li>Which of these two variables has bigger score?</li>\n<li>Round it to 2 decimal digits using round(score, 2)</li>\n</ul> \n</div>\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T10:32:33.230859Z","iopub.execute_input":"2021-09-17T10:32:33.231261Z","iopub.status.idle":"2021-09-17T10:32:33.25253Z","shell.execute_reply.started":"2021-09-17T10:32:33.231221Z","shell.execute_reply":"2021-09-17T10:32:33.25177Z"},"id":"2XGgZimiR8Xa"}},{"cell_type":"code","source":"\"\"\"Mutual Information\"\"\"\ndef mutual_info_bin_score(series):\n    return mutual_info_score(series, df_full_train.above_average)\n\nmi = df_full_train[categorical].apply(mutual_info_bin_score)\nmi.round(2).sort_values(ascending=False)    ","metadata":{"id":"9P0OJjcdR8Xa","outputId":"b383b1d6-bfae-4e25-ce7d-32f2596d24ce","execution":{"iopub.status.busy":"2021-09-29T12:14:26.477581Z","iopub.execute_input":"2021-09-29T12:14:26.477866Z","iopub.status.idle":"2021-09-29T12:14:26.602151Z","shell.execute_reply.started":"2021-09-29T12:14:26.477808Z","shell.execute_reply":"2021-09-29T12:14:26.601281Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:white\">Variable room_type has the biggest score = 0.14</div>","metadata":{"id":"5msozvRpfCcE"}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 4:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Now let's train a logistic regression</li>\n<li>Remember that we have two categorical variables in the data. Include them using one-hot encoding.</li>\n<li>Fit the model on the training dataset.</li>\n<ul>\n<li>To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:</li> \n<li>model = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)</li>\n</ul>     \n<li>Calculate the accuracy on the validation dataset and rount it to 2 decimal digits.</li>\n</ul> \n</div>","metadata":{"id":"dbh2u7IjR8Xb"}},{"cell_type":"code","source":"\"\"\"Taking Care of Categorical variables\"\"\"\ny_train = (df_train['price'] >= 152).values.astype(int)\ny_val = (df_val['price'] >= 152).values.astype(int)\ny_test = (df_test['price'] >= 152).values.astype(int)\n\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\nnumerical.remove('price')\n\ndecision = (y_val >= 152).astype(int)\n","metadata":{"id":"lnbwycjcR8Xb","execution":{"iopub.status.busy":"2021-09-29T12:14:26.605228Z","iopub.execute_input":"2021-09-29T12:14:26.606082Z","iopub.status.idle":"2021-09-29T12:14:26.745589Z","shell.execute_reply.started":"2021-09-29T12:14:26.606034Z","shell.execute_reply":"2021-09-29T12:14:26.744955Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(features):\n    # one-hot encoding datasets\n    dv = DictVectorizer(sparse=False)\n\n    train_dict = df_train[features].to_dict(orient='records')\n    val_dict = df_val[features].to_dict(orient='records')\n\n    X_train = dv.fit_transform(train_dict)\n    X_val = dv.transform(val_dict)\n\n    \"\"\"Fitting the Model on Training Set\"\"\"\n    model = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\n    model.fit(X_train, y_train)\n\n    \"\"\"Using the model on validation\"\"\"\n    y_pred = model.predict_proba(X_val)[:,1]\n\n    \"\"\"Setting up Decision Threshold to 0.5\"\"\"\n    decision = (y_pred >= 0.5)\n\n    \"\"\"Calculating accuracy\"\"\"\n    accuracy = (y_val == decision).mean()\n    \n\n    df_pred = pd.DataFrame()\n    df_pred['probability'] = y_pred\n    df_pred['prediction'] = decision\n    df_pred['actual'] = y_val\n    df_pred['correct'] = df_pred.prediction == df_pred.actual\n    return accuracy, df_pred\nacc, df_pred = calculate_accuracy(numerical+categorical)    \n\nprint(acc)\ndf_pred.head()","metadata":{"id":"M6oodqw1nP4z","outputId":"10a605af-22ff-4a44-f858-08b43b045f59","execution":{"iopub.status.busy":"2021-09-29T12:14:26.746963Z","iopub.execute_input":"2021-09-29T12:14:26.747715Z","iopub.status.idle":"2021-09-29T12:14:27.958543Z","shell.execute_reply.started":"2021-09-29T12:14:26.747651Z","shell.execute_reply":"2021-09-29T12:14:27.957746Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"all_vars_accuracy,_ = calculate_accuracy(numerical+categorical)\nall_vars_accuracy.round(2)","metadata":{"id":"44WxOW65-ZCs","outputId":"4a032839-add3-4286-95a6-39df08c6a141","execution":{"iopub.status.busy":"2021-09-29T12:14:27.959791Z","iopub.execute_input":"2021-09-29T12:14:27.960941Z","iopub.status.idle":"2021-09-29T12:14:29.135935Z","shell.execute_reply.started":"2021-09-29T12:14:27.960898Z","shell.execute_reply":"2021-09-29T12:14:29.135126Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 5:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>We have 9 features: 7 numerical features and 2 categorical.</li>\n<li>Let's find the least useful one using the feature elimination technique.</li>\n<li>Train a model with all these features (using the same parameters as in Q4).</li>\n<li>Now exclude each feature from this set and train a model without it. Record the accuracy for each model.</li>\n<li>For each feature, calculate the difference between the original accuracy and the accuracy without the feature.</li>\n<li>Which of following feature has the smallest difference? </li>\n<ul>\n<li>neighbourhood_group</li> \n<li>room_type</li>\n<li>number_of_reviews</li> \n<li>reviews_per_month</li>\n</ul> \n</ul>\n</div>\n","metadata":{"id":"QQj9ehf8R8Xb"}},{"cell_type":"code","source":"useful_features = numerical + categorical\ndiff = {}\nfor i in useful_features:\n  features = useful_features.copy()\n  features.remove(i)\n  acc,_ = calculate_accuracy(features)\n  diff[\"Difference in accuracy when removing %s\"%i] = all_vars_accuracy - acc\n\ndiff","metadata":{"id":"7FP2tjp6zb70","outputId":"eb668e68-0973-4be1-87e8-3cd963617e06","execution":{"iopub.status.busy":"2021-09-29T12:14:29.137170Z","iopub.execute_input":"2021-09-29T12:14:29.137906Z","iopub.status.idle":"2021-09-29T12:14:38.607767Z","shell.execute_reply.started":"2021-09-29T12:14:29.137861Z","shell.execute_reply":"2021-09-29T12:14:38.606891Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(diff.values(), index = diff.keys()).rename({0: 'differences'}, axis = 1).sort_values(by = 'differences', ascending = True).style.background_gradient('Reds')","metadata":{"id":"NO3t4J5nR8Xb","outputId":"336992df-6d2d-450e-a9f0-5c251feb45ea","execution":{"iopub.status.busy":"2021-09-29T12:14:38.609155Z","iopub.execute_input":"2021-09-29T12:14:38.609645Z","iopub.status.idle":"2021-09-29T12:14:38.635142Z","shell.execute_reply.started":"2021-09-29T12:14:38.609598Z","shell.execute_reply":"2021-09-29T12:14:38.634149Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Smallest Difference:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>number_of_reviews (0.000895)</li> \n</ul></div>","metadata":{"id":"BtCfpXQpUyG1"}},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 6:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>For this question, we'll see how to use a linear regression model from Scikit-Learn</li>\n<li>We'll need to use the original column 'price'. Apply the logarithmic transformation to this column.</li>\n<li>Fit the Ridge regression model on the training data.</li>\n<li>This model has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10]</li>\n<li>Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.</li>\n</ul>\n</div>\n","metadata":{"id":"Mx817--5FAEz"}},{"cell_type":"code","source":"\"\"\"Preparing Data for Linear Regression with Price Variable included\"\"\"\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\n\"\"\"Creating List of Numerical and Categorical columns\"\"\"\ncategorical = [col for col in df.columns if df[col].dtype == 'object']\nnumerical = [col for col in df.columns if col not in categorical]\n\n\"\"\"Apply the log transformation to the price variable using the np.log1p() function.\"\"\"\ny_train = np.log1p(df_train['price'].values)\ny_val = np.log1p(df_val['price'].values)\ny_test = np.log1p(df_test['price'].values)\n\n\n\"\"\"Make sure that the target value ('price') is not in your dataframe.\"\"\"\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\n\"\"\"Taking care of Categorical Variables\"\"\"\ndv = DictVectorizer(sparse=False)\nfeatures = categorical + numerical\nfeatures.remove('price')\ntrain_dict = df_train[features].to_dict(orient='records')\nval_dict = df_val[features].to_dict(orient='records')\n\nX_train = dv.fit_transform(train_dict)\nX_val = dv.transform(val_dict)","metadata":{"id":"aCbCagCZFGCm","execution":{"iopub.status.busy":"2021-09-29T12:14:38.636778Z","iopub.execute_input":"2021-09-29T12:14:38.637628Z","iopub.status.idle":"2021-09-29T12:14:39.498958Z","shell.execute_reply.started":"2021-09-29T12:14:38.637582Z","shell.execute_reply":"2021-09-29T12:14:39.497920Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"scores = {}\nfor alpha in [0, 0.01, 0.1, 1, 10]:\n    model = Ridge(alpha=alpha, random_state= 42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    scores[alpha] = score.round(3)\n    print(\"RMSE with alpha = %s and not rounding to 3 digits: %s\"%(alpha, score) )\nprint(\" \\nRMSE with rounding off to 3 digits\")\nprint(scores)\n\n","metadata":{"id":"g_BQFgFqFfNa","outputId":"ec9ef0b8-c4e8-4caf-faf6-70994768d41e","execution":{"iopub.status.busy":"2021-09-29T12:14:39.500108Z","iopub.execute_input":"2021-09-29T12:14:39.500339Z","iopub.status.idle":"2021-09-29T12:14:39.590292Z","shell.execute_reply.started":"2021-09-29T12:14:39.500310Z","shell.execute_reply":"2021-09-29T12:14:39.589464Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(\"Table of RMSE rounded to 3 digits\")\npd.DataFrame(scores.values(), index = scores.keys()).rename({0: 'RMSE'}, axis = 1).sort_values(by = 'RMSE', ascending = True).style.background_gradient('Reds')","metadata":{"id":"E3cz3PzdFlPy","outputId":"cec75c11-1dc6-47bd-f497-3173e8151945","execution":{"iopub.status.busy":"2021-09-29T12:14:39.591685Z","iopub.execute_input":"2021-09-29T12:14:39.592632Z","iopub.status.idle":"2021-09-29T12:14:39.617993Z","shell.execute_reply.started":"2021-09-29T12:14:39.592585Z","shell.execute_reply":"2021-09-29T12:14:39.617183Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:white\">$ \\large alpha$ that leads to the best RMSE: 0</div>","metadata":{"id":"ASMloPTwVTnU"}}]}